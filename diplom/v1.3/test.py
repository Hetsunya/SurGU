import torch
import torchvision.transforms as transforms
import torchvision.models as models
import cv2
import numpy as np
from PIL import Image

# –ü—É—Ç–∏
# MODEL_PATH = "emotion_model.pth"  # –§–∞–π–ª —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
MODEL_PATH = "best_model.pth"  # –§–∞–π–ª —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
CLASS_NAMES = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']  # –ö–ª–∞—Å—Å—ã

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = models.resnet18(weights=None)  # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
model.fc = torch.nn.Linear(model.fc.in_features, len(CLASS_NAMES))  # –ú–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
model.load_state_dict(torch.load(MODEL_PATH, map_location=device))  # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
model = model.to(device)
model.eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è


# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def predict_emotion(image_path):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    img = Image.open(image_path).convert("RGB")

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    transform = transforms.Compose([
        transforms.Resize((96, 96)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    img_tensor = transform(img).unsqueeze(0).to(device)  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    with torch.no_grad():
        output = model(img_tensor)
        _, predicted = torch.max(output, 1)

    emotion = CLASS_NAMES[predicted.item()]
    return emotion


# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ
image_path = "test/img_9.png"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
emotion = predict_emotion(image_path)
print(f"üü¢ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è: {emotion}")
