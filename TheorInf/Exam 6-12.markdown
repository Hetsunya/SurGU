## Билет 7: Информационные характеристики дискретного канала связи

Дискретный канал связи — это система передачи информации, в которой входные и выходные сигналы представляют собой дискретные символы из конечных алфавитов. Основные информационные характеристики включают:

### 1. **Пропускная способность канала**
Пропускная способность канала $C$ — максимальная скорость передачи информации с заданной достоверностью, измеряемая в битах на символ или битах в секунду. По теореме Шеннона:
$$
C = \max_{p(x)} I(X;Y),
$$
где $I(X;Y)$ — взаимная информация между входом $X$ и выходом $Y$, а максимум берется по всем возможным распределениям входных символов $p(x)$.

### 2. **Взаимная информация**
Взаимная информация $I(X;Y)$ характеризует количество информации о входном сигнале, переданное через канал:
$$
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X),
$$
где:
- $H(X)$ — энтропия входного алфавита,
- $H(Y)$ — энтропия выходного алфавита,
- $H(X|Y)$ — условная энтропия входа при заданном выходе,
- $H(Y|X)$ — условная энтропия выхода при заданном входе.

### 3. **Энтропия и условная энтропия**
- **Энтропия** $H(X)$ измеряет неопределенность входного алфавита:
$$
H(X) = -\sum_{x \in X} p(x) \log_2 p(x).
$$
- **Условная энтропия** $H(Y|X)$ характеризует неопределенность выхода при известном входе:
$$
H(Y|X) = -\sum_{x \in X} p(x) \sum_{y \in Y} p(y|x) \log_2 p(y|x).
$$

### 4. **Матрица переходных вероятностей**
Для дискретного канала без памяти матрица переходных вероятностей $p(y|x)$ описывает вероятность получения выходного символа $y$ при передаче входного символа $x$.

### 5. **Шум и потери информации**
Шум увеличивает $H(X|Y)$. Потери информации связаны с разностью между $H(X)$ и $I(X;Y)$.

### 6. **Типы каналов**
- **Канал без шума**: $H(X|Y) = 0$, $I(X;Y) = H(X)$.
- **Канал с потерями**: $I(X;Y) < H(X)$.
- **Бинарный симметричный канал (BSC)**: вероятность ошибки $p$, $p(y \neq x) = p$.
- **Канал с потерями**: символ либо передается корректно, либо теряется.

### Пример
Для BSC с вероятностью ошибки $p$:
$$
H(Y|X) = H_b(p) = -p \log_2 p - (1-p) \log_2 (1-p),
$$
где $H_b(p)$ — бинарная энтропия. Пропускная способность:
$$
C = 1 - H_b(p).
$$

---

## Билет 8: Эффективное кодирование. Основная теорема о кодировании

### Эффективное кодирование
Эффективное кодирование минимизирует среднюю длину кодового слова при однозначном декодировании.

#### Основные понятия:
- **Кодовое слово**: последовательность символов, представляющая символ алфавита.
- **Средняя длина кода**:
$$
L = \sum_{i=1}^n p_i l_i,
$$
где $p_i$ — вероятность символа $i$, $l_i$ — длина его кодового слова.
- **Эффективность кода**:
$$
\eta = \frac{H(X)}{L},
$$
где $H(X)$ — энтропия источника.

### Основная теорема о кодировании (Шеннона)
**Формулировка**: Для источника без памяти с энтропией $H(X)$ можно построить код со средней длиной $L$, такой что:
$$
H(X) \leq L < H(X) + 1,
$$
если код однозначно декодируем. Если $L < H(X)$, такой код невозможен.

#### Доказательство (идея):
- Энтропия $H(X)$ задает нижнюю границу.
- Неравенство Крафта для однозначно декодируемого кода:
$$
\sum_{i=1}^n 2^{-l_i} \leq 1,
$$
где $l_i$ — длины кодовых слов.

### Пример
Для алфавита $\{A, B, C\}$ с $p(A) = 0.5$, $p(B) = 0.25$, $p(C) = 0.25$:
$$
H(X) = -0.5 \log_2 0.5 - 2 \cdot 0.25 \log_2 0.25 = 1.5 \text{ бит}.
$$
Код Хаффмана: $A \to 0$, $B \to 10$, $C \to 11$. Средняя длина:
$$
L = 0.5 \cdot 1 + 0.25 \cdot 2 + 0.25 \cdot 2 = 1.5 \text{ бит}.
$$
Эффективность: $\eta = \frac{1.5}{1.5} = 1$.

---

## Билет 9: Методы эффективного кодирования некоррелированной последовательности знаков

Некоррелированная последовательность — выход источника без памяти, где символы независимы. Методы кодирования:

### 1. **Код Шеннона-Фано**
- Символы делятся на группы с равными суммами вероятностей, присваиваются биты 0 или 1.
- Не всегда оптимален.

### 2. **Код Хаффмана**
- Строится бинарное дерево, где листья — символы, кодовые слова — пути от корня.
- Алгоритм:
  1. Создать узлы с вероятностями символов.
  2. Объединить два узла с минимальными вероятностями.
  3. Повторять до одного узла.
  4. Присвоить 0/1 для ребер.
- Оптимальный префиксный код.

### 3. **Арифметическое кодирование**
- Кодирует последовательность как число в $[0, 1)$, сужая интервал по вероятностям символов.
- Приближается к энтропии, сложнее в реализации.

### 4. **Кодирование блоков**
- Кодирует группы символов, снижая разрыв между $L$ и $H(X)$.

---

## Билет 10: Свойство префиксности эффективных кодов

### Определение
Код **префиксный**, если ни одно кодовое слово не является префиксом другого. Обеспечивает мгновенное декодирование.

#### Пример:
- Префиксный: $\{0, 10, 11\}$.
- Непрефиксный: $\{0, 01, 10\}$.

### Свойства
1. **Однозначное декодирование**.
2. **Неравенство Крафта**:
$$
\sum_{i=1}^n 2^{-l_i} \leq 1.
$$
3. **Оптимальность**: эффективные коды (например, Хаффмана) префиксны.

### Пример
Для $\{A, B, C\}$ с $p = \{0.5, 0.25, 0.25\}$:
- Код Хаффмана: $A \to 0$, $B \to 10$, $C \to 11$.
- Неравенство Крафта:
$$
2^{-1} + 2^{-2} + 2^{-2} = 0.5 + 0.25 + 0.25 = 1.
$$

---

## Билет 11: Помехоустойчивое кодирование. Теорема Шеннона. Общие принципы построения помехоустойчивых кодов

### Помехоустойчивое кодирование
Добавляет избыточность для обнаружения и исправления ошибок.

#### Понятия:
- **Кодовая скорость**:
$$
R = \frac{k}{n},
$$
где $k$ — информационные символы, $n$ — длина кодового слова.
- **Кодовое расстояние** $d$: минимальное число различий между кодовыми словами.

### Теорема Шеннона
Для канала с пропускной способностью $C$ и кодовой скоростью $R < C$ существует код со сколь угодно малой вероятностью ошибки при увеличении $n$.

### Принципы построения
1. Добавление избыточности.
2. Увеличение $d$.
3. Блочное кодирование (Хэмминга, Рида-Соломона).
4. Сверточное кодирование.
5. Случайное кодирование.

---

## Билет 12: Связь корректирующей способности кода с кодовым расстоянием. Показатели качества корректирующего кода

### Связь корректирующей способности с кодовым расстоянием
**Корректирующая способность** $t$ — максимальное число исправляемых ошибок:
$$
t = \lfloor \frac{d-1}{2} \rfloor,
$$
где $d$ — кодовое расстояние.

#### Пример:
- Код Хэмминга: $d = 3$, $t = \lfloor \frac{3-1}{2} \rfloor = 1$.
- Код с $d = 5$: $t = \lfloor \frac{5-1}{2} \rfloor = 2$.

### Показатели качества
1. **Корректирующая способность** $t$.
2. **Кодовое расстояние** $d$.
3. **Кодовая скорость** $R = \frac{k}{n}$.
4. **Вероятность ошибки** после декодирования.
5. **Сложность кодирования/декодирования**.
6. **Избыточность**: $n - k$.

#### Пример:
Код Рида-Соломона ($n=255$, $k=223$, $d=33$):
- $t = \lfloor \frac{33-1}{2} \rfloor = 16$.
- $R = \frac{223}{255} \approx 0.875$.